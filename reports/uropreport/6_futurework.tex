\chapter{Future Work}
\label{futurework}
\paragraph{}
The immediate extension to this system would probably the bigram feature described by \cite{pederson}. Of course, we can adopt other forms of surface lexical features, like collocations, which is also featured in IMS's system. At this point, our system is a rather ``blind'' system, for it does not have any information about part-of-speech in the training and testing sentences. Hence the possible course of action would be include POS tagging features into the system, so that we would be able to attain higher accuracy.

\paragraph{}
Our analysis on the amount of overlap in the corpus, though it showed that it played a part in the performance, it also showed the weakness of the system's inability to process \textit{unknown} words to the system. An immediate course of action from this point would probably to expand the training data set, other than only using \textsc{SemCor}. Since our system captures training instances by sentences, we could possibly \textit{learn} from WordNet itself, as WordNet do provide sample sentences for most of the words in its inventory. The advantage in this move is that it would probably increase the system's WSD accuracies. However, it also increases the system's dependency on a lexical database. Increasing the size of training corpus would directly increase the size of the model and conflict with one of our goals | to reduce the size of model in order to make the system portable. Therefore, in the longer term, we wish to introduce some \textit{unsupervised} characteristics to the system, such that it would be less dependent on any back-end resources.
